{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soumiksinha2004-hash/ml/blob/main/Scorecard_project_(3).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import packages ::"
      ],
      "metadata": {
        "id": "aJZgJP3dCOqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xlsxwriter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktVGWSH0OELp",
        "outputId": "cc41f2e3-1e7c-412b-b6a4-5be5e50adf11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xlsxwriter\n",
            "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
            "Downloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, pandas as pd\n",
        "import os\n",
        "import re"
      ],
      "metadata": {
        "id": "zAWpLKjIAqLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display all columns and values in a dataframe\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)"
      ],
      "metadata": {
        "id": "EfvmN4O2a9de"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Change the directory :::"
      ],
      "metadata": {
        "id": "n3TTzBhpCpox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/drive/MyDrive/Scorecard_project\")"
      ],
      "metadata": {
        "id": "nWk9Oh5cCVHA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "f73116b1-b8b1-4934-bd46-8b0ed672a573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Scorecard_project'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1111601873.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Scorecard_project\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Scorecard_project'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "SwBEAGgaC2p7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the data :::"
      ],
      "metadata": {
        "id": "dKheFOS0C6qj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cdf = pd.read_excel(\"cibil_data_with_target.xlsx\")\n",
        "cdf.head()"
      ],
      "metadata": {
        "id": "p1EzOEAuC3gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cdf.target1.value_counts())"
      ],
      "metadata": {
        "id": "7Ml0-CYTEEDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(cdf.drop('target1', axis=1), cdf['target1'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a 'tag' column in the original DataFrame\n",
        "cdf['tag'] = 'train'\n",
        "cdf.loc[X_test.index, 'tag'] = 'test'\n",
        "\n",
        "# Display the first few rows with the new 'tag' column\n",
        "display(cdf.head())\n",
        "\n",
        "# Display the value counts for the 'tag' column to verify the split\n",
        "display(cdf['tag'].value_counts())"
      ],
      "metadata": {
        "id": "Hf9Vmimd7t2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA :::"
      ],
      "metadata": {
        "id": "i2kpncE6IKg8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Univariate analysis :::"
      ],
      "metadata": {
        "id": "iyPvoTupJAeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missing_percentage = cdf[cdf['tag'] == 'train'].isnull().sum() / len(cdf[cdf['tag'] == 'train']) * 100\n",
        "missing_percentage_df = pd.DataFrame({'Missing Percentage': missing_percentage})\n",
        "\n",
        "numerical_cols = [i for i in cdf.columns if cdf[i].nunique()>20]\n",
        "categorical_cols = [i for i in cdf.columns if cdf[i].nunique()<=20]\n",
        "\n",
        "\n",
        "numerical_stats = cdf[cdf['tag'] == 'train'][numerical_cols].describe(percentiles=[.25, .50, .75, .99]).T[['25%', '50%', '75%', '99%']]\n",
        "numerical_stats['variance'] = cdf[cdf['tag'] == 'train'][numerical_cols].var()\n",
        "\n",
        "\n",
        "# Combine the results\n",
        "combined_stats = missing_percentage_df.join(numerical_stats, how='left')\n",
        "\n",
        "# Add feature types\n",
        "combined_stats['Feature Type'] = 'Numerical'\n",
        "combined_stats.loc[categorical_cols, 'Feature Type'] = 'Categorical'\n",
        "combined_stats = combined_stats.reset_index()\n",
        "combined_stats.to_excel(\"univariate_analysis.xlsx\", index=False)\n",
        "display(combined_stats.sort_values(by='Missing Percentage', ascending=False))"
      ],
      "metadata": {
        "id": "ZIZ5SQo7IYOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_features = list(combined_stats[combined_stats['Missing Percentage']<71]['index'])\n",
        "\n",
        "selected_features = [i for i in selected_features if i not in ['num_dbt', 'num_lss']]"
      ],
      "metadata": {
        "id": "rn4bgMgAI4m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(selected_features)"
      ],
      "metadata": {
        "id": "8X4D5x0eQbnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_categorical_features = [i for i in selected_features if i in categorical_cols]\n",
        "selected_categorical_features"
      ],
      "metadata": {
        "id": "k-0kS9Dx-M9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for col in selected_categorical_features:\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    ax = sns.countplot(x=col, data=cdf[cdf['tag'] == 'train'], palette='viridis') # Use 'viridis' palette for different colors\n",
        "    plt.title(f'Distribution of {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "    total = len(cdf[col].dropna()) # Calculate total count for percentage\n",
        "    for p in ax.patches:\n",
        "        percentage = '{:.1f}%'.format(100 * p.get_height() / total)\n",
        "        x = p.get_x() + p.get_width() / 2\n",
        "        y = p.get_height()\n",
        "        ax.annotate(percentage, (x, y), ha='center', va='bottom')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "hxM_2bSJ-aip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bivariate analysis :::"
      ],
      "metadata": {
        "id": "rZxo_tpVgJMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features_to_remove = ['num_deliq_6mts','num_deliq_6_12mts','num_sub_6mts','num_sub_12mts','num_dbt_6mts','num_dbt_12mts','num_lss_6mts','num_lss_12mts']\n",
        "selected_features = [i for i in selected_features if i not in features_to_remove]\n",
        "len(selected_features)"
      ],
      "metadata": {
        "id": "pt5ETn6LDJuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cdf1 = cdf[selected_features]\n",
        "cdf1.head()"
      ],
      "metadata": {
        "id": "M4cPrqjfifAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xlsxwriter"
      ],
      "metadata": {
        "id": "xKSUsSK_KjWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cdf1.GENDER.value_counts()"
      ],
      "metadata": {
        "id": "SK5yizGSqQDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Encoding :::"
      ],
      "metadata": {
        "id": "240BYEXGUGkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OrdinalEncoder, LabelEncoder\n",
        "\n",
        "ed_order = ['SSC', '12TH', 'GRADUATE', 'UNDER GRADUATE', 'POST-GRADUATE','PROFESSIONAL', 'OTHERS']\n",
        "\n",
        "ordencoder = OrdinalEncoder(categories=[ed_order])\n",
        "lblencoder = LabelEncoder()\n",
        "stdscaler = StandardScaler()\n",
        "\n",
        "\n",
        "# cdf2['ED_coded'] = ordencoder.fit_transform(cdf2[['EDUCATION']])\n",
        "cdf1['last_prod_enq2_coded'] = lblencoder.fit_transform(cdf1['last_prod_enq2'])\n",
        "cdf1['first_prod_enq2_coded'] = lblencoder.fit_transform(cdf1['first_prod_enq2'])\n",
        "\n",
        "cdf1['MARITALSTATUS_coded'] = 1\n",
        "cdf1.loc[cdf1['MARITALSTATUS'] == 'Single', 'MARITALSTATUS_coded'] = 0\n",
        "\n",
        "cdf1['education_coded'] = ordencoder.fit_transform(cdf1[['EDUCATION']])\n",
        "\n",
        "cdf1['gender_coded'] = 1\n",
        "cdf1.loc[cdf1['GENDER'] == 'F', 'gender_coded'] = 0\n",
        "\n",
        "\n",
        "\n",
        "cdf1 = cdf1.drop(['last_prod_enq2','first_prod_enq2','MARITALSTATUS','EDUCATION','GENDER'], axis=1)\n",
        "\n",
        "\n",
        "cdf1.head()"
      ],
      "metadata": {
        "id": "Ah0kr3NhT6MP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Correlation analysis :::"
      ],
      "metadata": {
        "id": "rb6EZ9yviyZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Select only numerical columns for correlation analysis\n",
        "numerical_cols = cdf1.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = cdf1[numerical_cols].corr()\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Cross-Correlation Heatmap of Numerical Features')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t9bDsF0FgphB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feat_to_remove = ['max_recent_level_of_deliq','max_deliq_12mts','num_times_30p_dpd','num_std','num_std_6mts',\n",
        "                  'CC_enq_L6m','CC_enq_L12m','PL_enq_L6m','PL_enq_L12m','enq_L6m','enq_L3m','pct_PL_enq_L6m_of_ever','pct_CC_enq_L6m_of_ever']\n",
        "\n",
        "filtered_cols = [i for i in cdf1.columns if i not in feat_to_remove]\n",
        "cdf1 = cdf1[filtered_cols].copy()\n",
        "cdf1.shape"
      ],
      "metadata": {
        "id": "UGzvh95Pg-0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cdf1.columns"
      ],
      "metadata": {
        "id": "VJYNaRMxkMPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def calculate_iv_woe(df, target):\n",
        "    \"\"\"\n",
        "    Calculates the Information Value (IV) and Weight of Evidence (WOE) for all features in a DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "        target (str): The name of the target column (must be binary: 0 or 1).\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - pd.DataFrame: A DataFrame containing the IV for each feature.\n",
        "            - dict: A dictionary where keys are feature names and values are DataFrames containing WOE details.\n",
        "    \"\"\"\n",
        "    iv_values = {}\n",
        "    woe_reports = {}\n",
        "    for col in df.columns:\n",
        "        if col == target:\n",
        "            continue\n",
        "\n",
        "        # For numerical features, bin the data\n",
        "        if df[col].nunique() > 10:\n",
        "            # Create bins for numerical features. Adjust the number of bins as needed.\n",
        "            try:\n",
        "                bins = pd.qcut(df[col], q=10, labels=False, duplicates='drop')\n",
        "                data = pd.DataFrame({'bins': bins, 'target': df[target]})\n",
        "                data = data.groupby('bins')['target'].agg(['count', 'sum']).reset_index()\n",
        "                data.columns = ['bin', 'total', 'good']\n",
        "                data['bad'] = data['total'] - data['good']\n",
        "                # data.columns = ['bin', 'total', 'bad']\n",
        "                # data['good'] = data['total'] - data['bad']\n",
        "            except (ValueError, KeyError):\n",
        "                 iv_values[col] = 0 # IV is 0 if binning fails\n",
        "                 woe_reports[col] = pd.DataFrame() # Empty DataFrame for WOE if binning fails\n",
        "                 continue\n",
        "\n",
        "        # For categorical features, use the categories as groups\n",
        "        else:\n",
        "            data = df.groupby(col)[target].agg(['count', 'sum']).reset_index()\n",
        "            data.columns = ['bin', 'total', 'good']\n",
        "            data['bad'] = data['total'] - data['good']\n",
        "            # data.columns = ['bin', 'total', 'bad']\n",
        "            # data['good'] = data['total'] - data['bad']\n",
        "\n",
        "\n",
        "        # Calculate WOE and IV\n",
        "        data['bad_rate'] = data['bad'] / data['bad'].sum()\n",
        "        data['good_rate'] = data['good'] / data['good'].sum()\n",
        "\n",
        "        # Avoid division by zero or log(0)\n",
        "        data['woe'] = np.log((data['good_rate'] + 1e-6) / (data['bad_rate'] + 1e-6))\n",
        "        data['iv'] = (data['good_rate'] - data['bad_rate']) * data['woe']\n",
        "\n",
        "        iv_values[col] = data['iv'].sum()\n",
        "        woe_reports[col] = data[['bin', 'total', 'good', 'bad', 'good_rate', 'bad_rate', 'woe', 'iv']]\n",
        "\n",
        "\n",
        "    iv_df = pd.DataFrame.from_dict(iv_values, orient='index', columns=['IV'])\n",
        "    iv_df.index.name = 'Feature'\n",
        "    iv_df = iv_df.sort_values(by='IV', ascending=False).reset_index()\n",
        "\n",
        "    return iv_df, woe_reports\n",
        "\n",
        "# Example usage (assuming your DataFrame is cdf and target is 'target1')\n",
        "iv_report, woe_reports = calculate_iv_woe(cdf1[cdf1['tag']=='train'], 'target1')\n",
        "display(iv_report)\n",
        "# To display the WOE report for a specific feature, e.g., 'AGE':\n",
        "# display(woe_reports['AGE'])"
      ],
      "metadata": {
        "id": "o4yWNGFVfby-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf7aeca7"
      },
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "def calculate_vif(df):\n",
        "    \"\"\"\n",
        "    Calculates the Variance Inflation Factor (VIF) for numerical features in a DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing the VIF for each numerical feature.\n",
        "    \"\"\"\n",
        "    numerical_df = df.select_dtypes(include=np.number).fillna(-99999) # Replace NaN with -99999\n",
        "\n",
        "    vif_data = pd.DataFrame()\n",
        "    vif_data[\"feature\"] = numerical_df.columns\n",
        "\n",
        "    vif_data[\"VIF\"] = [variance_inflation_factor(numerical_df.values, i)\n",
        "                       for i in range(numerical_df.shape[1])]\n",
        "\n",
        "    return vif_data.sort_values(by='VIF', ascending=False)\n",
        "\n",
        "# Example usage (assuming your DataFrame is cdf)\n",
        "vif_report = calculate_vif(cdf1[cdf1['tag']=='train'].drop(['target1'], axis=1))\n",
        "display(vif_report)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5y4F73D03aL"
      },
      "source": [
        "from scipy.stats import f_oneway\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def calculate_anova(df, target):\n",
        "    \"\"\"\n",
        "    Performs ANOVA tests for numerical features against a binary target variable.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "        target (str): The name of the binary target column (0 or 1).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing the F-value and P-value for each numerical feature.\n",
        "    \"\"\"\n",
        "    numerical_cols = [col for col in df.columns if df[col].nunique() > 20]\n",
        "\n",
        "    anova_results = {}\n",
        "    for col in numerical_cols:\n",
        "        if col != target:\n",
        "            # Handle potential NaN values by dropping them for the ANOVA test\n",
        "            groups = df.dropna(subset=[col, target]).groupby(target)[col].apply(list)\n",
        "            if len(groups) > 1: # ANOVA requires at least two groups\n",
        "                try:\n",
        "                    fvalue, pvalue = f_oneway(*groups)\n",
        "                    anova_results[col] = {'anova F-value': fvalue, 'anova P-value': pvalue}\n",
        "                except ValueError: # Handle cases where groups have no variance\n",
        "                    anova_results[col] = {'anova F-value': np.nan, 'anova P-value': np.nan}\n",
        "\n",
        "\n",
        "    anova_report = pd.DataFrame.from_dict(anova_results, orient='index')\n",
        "    anova_report.index.name = 'Feature'\n",
        "    return anova_report.sort_values(by='anova P-value').reset_index()\n",
        "\n",
        "# Example usage (assuming your DataFrame is cdf and target is 'target1')\n",
        "numerical_cols = [i for i in cdf1.columns if cdf1[i].nunique()>20]+['target1']\n",
        "anova_report = calculate_anova(cdf1.loc[cdf1['tag']=='train', numerical_cols], 'target1')\n",
        "display(anova_report)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as ss\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def cramers_v(x, y):\n",
        "    \"\"\"\n",
        "    Calculates Cramer's V for two categorical variables.\n",
        "\n",
        "    Args:\n",
        "        x (pd.Series): The first categorical variable.\n",
        "        y (pd.Series): The second categorical variable (target variable).\n",
        "\n",
        "    Returns:\n",
        "        float: The Cramer's V value.\n",
        "    \"\"\"\n",
        "    confusion_matrix = pd.crosstab(x, y)\n",
        "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
        "    n = confusion_matrix.sum().sum()\n",
        "    phi2 = chi2 / n\n",
        "    r, k = confusion_matrix.shape\n",
        "    # Corrected Cramer's V\n",
        "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
        "    rcorr = r - ((r-1)**2)/(n-1)\n",
        "    kcorr = k - ((k-1)**2)/(n-1)\n",
        "    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n",
        "\n",
        "# Example usage (assuming your DataFrame is cdf and target is 'target1')\n",
        "categorical_cols = [i for i in cdf1.columns if cdf1[i].nunique()<=20]+['target1']\n",
        "cramers_v_results = {col: cramers_v(cdf1[col], cdf1['target1']) for col in categorical_cols if col != 'target1'}\n",
        "cramers_v_report = pd.DataFrame.from_dict(cramers_v_results, orient='index', columns=[\"Cramer's V\"]).sort_values(by=\"Cramer's V\", ascending=False).reset_index()\n",
        "display(cramers_v_report)"
      ],
      "metadata": {
        "id": "ZiP8YD25iSlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bivariate_analysis = iv_report.merge(vif_report, left_on='Feature', right_on='feature', how='left').drop(['feature'], axis=1)\n",
        "bivariate_analysis = bivariate_analysis.merge(anova_report, left_on='Feature', right_on='Feature', how='left')\n",
        "bivariate_analysis = bivariate_analysis.merge(cramers_v_report, left_on='Feature', right_on='index', how='left').drop(['index'], axis=1)\n",
        "\n",
        "bivariate_analysis.head(3)"
      ],
      "metadata": {
        "id": "aNYBi0u41zes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bivariate_analysis['type'] = 'numerical'\n",
        "bivariate_analysis.loc[bivariate_analysis['Feature'].isin(categorical_cols), 'type'] = 'categorical'\n",
        "bivariate_analysis.to_excel(\"bivariate_analysis.xlsx\", index=False)\n",
        "bivariate_analysis.head()"
      ],
      "metadata": {
        "id": "Yj4qb7Ocf-4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cdf1 = cdf1[l+['target1','tag']]\n",
        "cdf1.shape"
      ],
      "metadata": {
        "id": "hh08YRKhpQXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Event rate analysis :::"
      ],
      "metadata": {
        "id": "dguxMhaoisn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def export_to_xl(df_name, var_name, writer, iterator=0, pd_var=None):\n",
        "\n",
        "    VAR_NAME = var_name[:31] if len(var_name) > 31 else var_name\n",
        "\n",
        "    from xlsxwriter.utility import xl_rowcol_to_cell\n",
        "\n",
        "    start_row = 16\n",
        "    start_col = 2 + (10 * iterator)\n",
        "    endrow = start_row + len(df_name)\n",
        "\n",
        "    chart_pos_var = xl_rowcol_to_cell(1, 2 + (10 * iterator))\n",
        "\n",
        "    cat_char_strt = xl_rowcol_to_cell(start_row+1, 3 + (10 * iterator), row_abs=True, col_abs=True)\n",
        "    cat_char_end = xl_rowcol_to_cell(endrow-1, 3 + (10 * iterator), row_abs=True, col_abs=True)\n",
        "\n",
        "    bp_nm_char = xl_rowcol_to_cell(start_row, 6 + (10 * iterator), row_abs=True, col_abs=True)\n",
        "    bp_val_char_strt = xl_rowcol_to_cell(start_row+1, 6 + (10 * iterator), row_abs=True, col_abs=True)\n",
        "    bp_val_char_end = xl_rowcol_to_cell(endrow-1, 6 + (10 * iterator), row_abs=True, col_abs=True)\n",
        "\n",
        "    lp_nm_char = xl_rowcol_to_cell(start_row, 7 + (10 * iterator), row_abs=True, col_abs=True)\n",
        "    lp_val_char_strt = xl_rowcol_to_cell(start_row+1, 7 + (10 * iterator), row_abs=True, col_abs=True)\n",
        "    lp_val_char_end = xl_rowcol_to_cell(endrow-1, 7 + (10 * iterator), row_abs=True, col_abs=True)\n",
        "\n",
        "    total_event_rate_strt = xl_rowcol_to_cell(start_row+1, 8 + (10 * iterator), row_abs=True, col_abs=True)\n",
        "    total_event_rate_end = xl_rowcol_to_cell(endrow-1, 8 + (10 * iterator), row_abs=True, col_abs=True)\n",
        "\n",
        "    bar_plot_name = \"=\" + VAR_NAME + \"!\" + bp_nm_char\n",
        "    bar_plot_cat = \"=\" + VAR_NAME + \"!\" + cat_char_strt + \":\" + cat_char_end\n",
        "    bar_plot_values = \"=\" + VAR_NAME + \"!\" + bp_val_char_strt + \":\" + bp_val_char_end\n",
        "\n",
        "    line_plot_name = \"=\" + VAR_NAME + \"!\" + lp_nm_char\n",
        "    line_plot_cat = \"=\" + VAR_NAME + \"!\" + cat_char_strt + \":\" + cat_char_end\n",
        "    line_plot_values = \"=\" + VAR_NAME + \"!\" + lp_val_char_strt + \":\" + lp_val_char_end\n",
        "\n",
        "    total_event_rate_values = \"=\" + VAR_NAME + \"!\" + total_event_rate_strt + \":\" + total_event_rate_end\n",
        "\n",
        "    # Convert the dataframe to an XlsxWriter Excel object.\n",
        "    df_name.to_excel(writer, sheet_name=VAR_NAME, startcol=start_col, startrow=start_row, index=False)\n",
        "\n",
        "    # Get the xlsxwriter workbook and worksheet objects.\n",
        "    workbook = writer.book\n",
        "    worksheet = writer.sheets[VAR_NAME]\n",
        "\n",
        "    perc_fmt = workbook.add_format({'num_format': '0.00%', 'align': 'center', 'valign': 'center'})\n",
        "    num_fmt = workbook.add_format({'num_format': '#,##0', 'align': 'center', 'valign': 'center'})\n",
        "    # char_fmt = workbook.add_format({'align': 'center', 'valign': 'center'})\n",
        "\n",
        "    # Create a (primary) column chart.\n",
        "    bar_plot = workbook.add_chart({'type': 'column'})\n",
        "\n",
        "    # Configure the data series for the primary chart.\n",
        "    bar_plot.add_series({\n",
        "        'name':       bar_plot_name,\n",
        "        'categories': bar_plot_cat,\n",
        "        'values':     bar_plot_values,\n",
        "        'fill': {\n",
        "            'color': '#C34B45'  # Red Accent 2\n",
        "        }\n",
        "    })\n",
        "\n",
        "    # Create a (secondary) line chart.\n",
        "    line_plot = workbook.add_chart({'type': 'line'})\n",
        "\n",
        "    # Configure the data series for the secondary chart.\n",
        "    # y2_axis = secondary Y axis.\n",
        "    line_plot.add_series({\n",
        "        'name':       line_plot_name,\n",
        "        'categories': line_plot_cat,\n",
        "        'values':     line_plot_values,\n",
        "        'y2_axis':    True,\n",
        "        'line': {\n",
        "            'color': '#FFA500',  # Orange\n",
        "            'dash_type': 'solid'\n",
        "        }\n",
        "    })\n",
        "\n",
        "    # Configure the data series for the Total_Event_Rate line chart.\n",
        "    line_plot.add_series({\n",
        "        'name':       'Overall_Event_Rate',\n",
        "        'categories': line_plot_cat,\n",
        "        'values':     total_event_rate_values,\n",
        "        'y2_axis':    True,\n",
        "        'line': {\n",
        "            'color': 'black',\n",
        "            'dash_type': 'dash'\n",
        "        }\n",
        "    })\n",
        "    # Combining the charts.\n",
        "    bar_plot.combine(line_plot)\n",
        "\n",
        "    # Adding chart title.\n",
        "    if pd_var:\n",
        "        title = var_name + \"_\" + pd_var\n",
        "    else:\n",
        "        title = var_name + \"_OVERALL\"\n",
        "\n",
        "    bar_plot.set_title({'name': title})\n",
        "    bar_plot.set_y_axis({'major_gridlines': {'visible': False}})\n",
        "    bar_plot.set_legend({'position': 'bottom'})\n",
        "\n",
        "    bold = workbook.add_format({'bold': 1})\n",
        "    worksheet.write('B1', '=HYPERLINK(\"#Summary!A1\",\"Back\")', bold)\n",
        "    bar_plot.combine(line_plot)\n",
        "\n",
        "    # Adding chart title.\n",
        "    if pd_var:\n",
        "        title = var_name + \"_\" + pd_var\n",
        "    else:\n",
        "        title = var_name + \"_OVERALL\"\n",
        "\n",
        "    bar_plot.set_title({'name': title})\n",
        "    bar_plot.set_y_axis({'major_gridlines': {'visible': False}})\n",
        "    bar_plot.set_legend({'position': 'bottom'})\n",
        "\n",
        "    bold = workbook.add_format({'bold': 1})\n",
        "    worksheet.write('B1', '=HYPERLINK(\"#Summary!A1\",\"Back\")', bold)\n",
        "    bar_plot.combine(line_plot)\n",
        "\n",
        "    # Adding chart title.\n",
        "    if pd_var:\n",
        "        title = var_name + \"_\" + pd_var\n",
        "    else:\n",
        "        title = var_name + \"_OVERALL\"\n",
        "\n",
        "    bar_plot.set_title({'name': title})\n",
        "    bar_plot.set_y_axis({'major_gridlines': {'visible': False}})\n",
        "    bar_plot.set_legend({'position': 'bottom'})\n",
        "\n",
        "    bold = workbook.add_format({'bold': 1})\n",
        "    worksheet.write('B1', '=HYPERLINK(\"#Summary!A1\",\"Back\")', bold)\n",
        "\n",
        "    range_strt = xl_rowcol_to_cell(start_row, 2 + (10 * iterator))\n",
        "    range_end = xl_rowcol_to_cell(endrow, 10 + (10 * iterator))\n",
        "    range_var = range_strt + \":\" + range_end\n",
        "\n",
        "    num_range = re.sub('[0-9]', '', xl_rowcol_to_cell(start_row, 4 + (10 * iterator))) + \":\" + re.sub('[0-9]', '', xl_rowcol_to_cell(start_row, 5 + (10 * iterator)))\n",
        "    perc_range = re.sub('[0-9]', '', xl_rowcol_to_cell(start_row, 6 + (10 * iterator))) + \":\" + re.sub('[0-9]', '', xl_rowcol_to_cell(start_row, 7 + (10 * iterator)))\n",
        "    perc_range2 = re.sub('[0-9]', '', xl_rowcol_to_cell(start_row, 8 + (10 * iterator))) + \":\" + re.sub('[0-9]', '', xl_rowcol_to_cell(start_row, 9 + (10 * iterator)))\n",
        "    num_range2 = re.sub('[0-9]', '', xl_rowcol_to_cell(start_row, 9 + (10 * iterator))) + \":\" + re.sub('[0-9]', '', xl_rowcol_to_cell(start_row, 10 + (10 * iterator)))\n",
        "    num_range3 = re.sub('[0-9]', '', xl_rowcol_to_cell(start_row, 10 + (10 * iterator))) + \":\" + re.sub('[0-9]', '', xl_rowcol_to_cell(start_row, 11 + (10 * iterator)))\n",
        "\n",
        "    border_format = workbook.add_format({\n",
        "        'border': 1,\n",
        "        'align': 'center',\n",
        "        'font_size': 10\n",
        "    })\n",
        "\n",
        "    worksheet.conditional_format(range_var, {'type': 'no_blanks', 'format': border_format})\n",
        "\n",
        "    # Insert the chart into the worksheet\n",
        "    worksheet.insert_chart(chart_pos_var, bar_plot)\n",
        "    worksheet.hide_gridlines(2)\n",
        "    worksheet.set_column(num_range, 10.25, num_fmt)\n",
        "    worksheet.set_column(perc_range, 10.25, perc_fmt)\n",
        "    worksheet.set_column(perc_range2, 10.25, perc_fmt)\n",
        "    worksheet.set_column(num_range2, 10.25, num_fmt)\n",
        "    worksheet.set_column(num_range3, 10.25, num_fmt)\n",
        "    worksheet.set_zoom(80)\n",
        "\n",
        "    # writer.save()  # saving is done outside of this loop\n"
      ],
      "metadata": {
        "id": "LIDNON_-HwtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for creating Character Variable Tables\n",
        "def char_var_tbl(df_chk_chars_concat, dep_var, i, writer1):\n",
        "    df_v1_EV = df_chk_chars_concat.groupby([i])[[dep_var, 'DF_RECS_CNT']].sum().reset_index()\n",
        "    df_v1_EV['POP_RATE'] = df_v1_EV['DF_RECS_CNT']/df_v1_EV['DF_RECS_CNT'].sum()\n",
        "    df_v1_EV['Event_Rate'] = df_v1_EV[dep_var]/df_v1_EV['DF_RECS_CNT']\n",
        "    df_v1_EV['Total_Event_Rate'] = df_v1_EV[dep_var].sum()/df_v1_EV['DF_RECS_CNT'].sum()\n",
        "    df_v1_EV['Variable'] = i\n",
        "    df_v1_EV.rename(columns={i: 'ATTRIBUTE'}, inplace=True)\n",
        "    df_v1_EV['MIN_VALUE'] = \"NA\"\n",
        "    df_v1_EV['MAX_VALUE'] = \"NA\"\n",
        "\n",
        "    # CALCULATING VALUES FOR GROUP TOTALS\n",
        "    columns = ['Variable', 'ATTRIBUTE', dep_var, 'DF_RECS_CNT', 'POP_RATE',\n",
        "              'Event_Rate', 'Total_Event_Rate', 'MAX_VALUE', 'MIN_VALUE']\n",
        "    data = [[i, \"Total\", df_v1_EV[dep_var].sum(), df_v1_EV['DF_RECS_CNT'].sum(),\n",
        "            df_v1_EV['POP_RATE'].sum(), df_v1_EV[dep_var].sum()/df_v1_EV['DF_RECS_CNT'].sum(),\n",
        "            \"NA\", \"NA\", \"NA\"]]\n",
        "\n",
        "    df_v1_EV_tot = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "    DF_CHAR = pd.concat([df_v1_EV, df_v1_EV_tot], axis=0, ignore_index=True)\n",
        "    DF_CHAR = DF_CHAR[['Variable', 'ATTRIBUTE', dep_var, 'DF_RECS_CNT', 'POP_RATE',\n",
        "                      'Event_Rate', 'Total_Event_Rate', 'MIN_VALUE', 'MAX_VALUE']]\n",
        "    return DF_CHAR\n"
      ],
      "metadata": {
        "id": "WO8-5xknIepd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def num_var_tbl(df_chk_nums_concat, dep_var, i, writer1):\n",
        "    rank_var = i + \"_rank\"\n",
        "    bins = pd.qcut(df_chk_nums_concat[i], 10, duplicates=\"drop\")\n",
        "\n",
        "    def format_interval(interval):\n",
        "        lower = interval.left\n",
        "        higher = interval.right\n",
        "\n",
        "        if lower >= pow(10, 6):\n",
        "            lower = f\"{str(round(lower / pow(10, 6), 1))}M\"\n",
        "        elif lower >= pow(10, 3):\n",
        "            lower = f\"{str(round(lower / pow(10, 3), 1))}K\"\n",
        "        else:\n",
        "            lower = str(round(lower, 1))\n",
        "\n",
        "        if higher >= pow(10, 6):\n",
        "            higher = f\"{str(round(higher / pow(10, 6), 1))}M\"\n",
        "        elif higher >= pow(10, 3):\n",
        "            higher = f\"{str(round(higher / pow(10, 3), 1))}K\"\n",
        "        else:\n",
        "            higher = str(round(higher, 1))\n",
        "\n",
        "        return f\"{lower} - {higher}\"\n",
        "\n",
        "    bins = bins.apply(format_interval)\n",
        "    df_chk_nums_concat[rank_var] = bins\n",
        "\n",
        "    df_v1_num_EV = df_chk_nums_concat.groupby(rank_var)[[dep_var, 'DF_RECS_CNT']].sum().reset_index()\n",
        "    df_v1_num_EV['POP_RATE'] = df_v1_num_EV['DF_RECS_CNT'] / df_v1_num_EV['DF_RECS_CNT'].sum()\n",
        "\n",
        "    df_v1_num_MIN_MAX = df_chk_nums_concat.groupby(rank_var).agg({i: ['min', 'max']}).reset_index()\n",
        "    df_v1_num_MIN_MAX.columns = df_v1_num_MIN_MAX.columns.droplevel(0)\n",
        "    df_v1_num_MIN_MAX.columns = [rank_var, 'MIN_VALUE', 'MAX_VALUE']\n",
        "\n",
        "    df_v1_num_CMP2 = pd.merge(left=df_v1_num_EV, right=df_v1_num_MIN_MAX, how='left', on=rank_var)\n",
        "    df_v1_num_CMP2['Event_Rate'] = df_v1_num_CMP2[dep_var] / df_v1_num_CMP2['DF_RECS_CNT']\n",
        "    df_v1_num_CMP2['Total_Event_Rate'] = df_v1_num_CMP2[dep_var].sum() / df_v1_num_CMP2['DF_RECS_CNT'].sum()\n",
        "    df_v1_num_CMP2['Variable'] = i\n",
        "    df_v1_num_CMP2.rename(columns={rank_var: 'ATTRIBUTE'}, inplace=True)\n",
        "\n",
        "    # CALCULATING VALUES FOR GROUP TOTALS\n",
        "    columns = ['Variable', 'ATTRIBUTE', dep_var, 'DF_RECS_CNT', 'POP_RATE',\n",
        "               'Event_Rate', 'Total_Event_Rate', 'MAX_VALUE', 'MIN_VALUE']\n",
        "    data = [[i, 'Total', df_v1_num_CMP2[dep_var].sum(), df_v1_num_CMP2['DF_RECS_CNT'].sum(),df_v1_num_CMP2['POP_RATE'].sum(),\n",
        "             df_v1_num_CMP2[dep_var].sum() / df_v1_num_CMP2['DF_RECS_CNT'].sum(),\n",
        "             \"NA\", \"NA\", \"NA\"]]\n",
        "\n",
        "    df_v1_num_CMP2_tot = pd.DataFrame(data, columns=columns)\n",
        "    # df_v1_num_CMP2['Total_Event_Rate'] = [df_v1_num_CMP2_tot['Total_Event_Rate']]*len(df_v1_num_CMP2)\n",
        "\n",
        "    DF_NUM = pd.concat([df_v1_num_CMP2, df_v1_num_CMP2_tot], axis=0, ignore_index=True)\n",
        "    DF_NUM = DF_NUM[['Variable', 'ATTRIBUTE', dep_var, 'DF_RECS_CNT',\n",
        "                     'POP_RATE', 'Event_Rate', 'Total_Event_Rate', 'MIN_VALUE', 'MAX_VALUE']]\n",
        "\n",
        "    return DF_NUM"
      ],
      "metadata": {
        "id": "g4dUvWB5JMuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function\n",
        "def bivariate(df_chk,depvar,output_file_nm,MODEL_PERIOD_FLAG=None):\n",
        "    from xlsxwriter.utility import xl_rowcol_to_cell\n",
        "\n",
        "    numerical_cols = [i for i in df_chk.columns if df_chk[i].nunique()>20]\n",
        "    if depvar not in numerical_cols:\n",
        "        numerical_cols.append(depvar)\n",
        "\n",
        "    categorical_cols = [i for i in df_chk.columns if df_chk[i].nunique()<=20]\n",
        "    if depvar in categorical_cols:\n",
        "        categorical_cols.remove(depvar)\n",
        "\n",
        "    df_chk_chars = df_chk[categorical_cols]\n",
        "    df_chk_nums = df_chk[numerical_cols]\n",
        "\n",
        "    # df_chk_chars = df_chk.select_dtypes(include = object)\n",
        "    # df_chk_nums = df_chk.select_dtypes(include = 'number')\n",
        "\n",
        "    print(\"all features:\",df_chk.columns)\n",
        "    print(\"number type features:\",df_chk_nums.columns)\n",
        "    print(\"character type features:\",df_chk_chars.columns)\n",
        "\n",
        "\n",
        "    max_cat = 10000\n",
        "    df_chk_depvar = df_chk[[depvar]]\n",
        "    if MODEL_PERIOD_FLAG :\n",
        "        df_chk_pdvar = df_chk[[MODEL_PERIOD_FLAG]]\n",
        "    else :\n",
        "        df_chk_pdvar = pd.DataFrame()\n",
        "    df_chk_pd_depvar = pd.concat([df_chk_depvar, df_chk_pdvar], axis=1)\n",
        "    try:\n",
        "        df_chk_nums.drop([depvar],axis=1,inplace=True)\n",
        "        # df_chk_chars.drop([depvar],axis=1,inplace=True)\n",
        "        pass\n",
        "    except ValueError:\n",
        "        pass\n",
        "\n",
        "    varlist_num = df_chk_nums.columns\n",
        "    varlist_char = df_chk_chars.columns\n",
        "\n",
        "    varlist_comb = df_chk_chars.columns.tolist()\n",
        "    for i1 in varlist_comb:\n",
        "        if df_chk_chars[i1].unique().shape[0]>=max_cat :\n",
        "            varlist_comb.remove(i1)\n",
        "\n",
        "    varlist_comb.extend(df_chk_nums.columns.tolist())\n",
        "    varlist_comb2 = l =  [None] * len(varlist_comb)\n",
        "\n",
        "    for i in np.arange(len(varlist_comb)):\n",
        "        link_name = varlist_comb[i][:31] if len(varlist_comb[i])>31 else varlist_comb[i]\n",
        "        varlist_comb2[i] = '=HYPERLINK(\"#'+link_name+'!A1\",\"'+link_name+'\")'\n",
        "\n",
        "    summ_tab_df = pd.DataFrame(list(zip(varlist_comb, varlist_comb2)),\n",
        "                columns=['Variable Name','Variable Sheet Link'])\n",
        "    #summ_tab_df = pd.DataFrame(varlist_comb,columns=[\"Variable Name\"])\n",
        "    #print(summ_tab_df[\"Variable Name\"])\n",
        "\n",
        "    df_chk_chars_concat = pd.concat([df_chk_chars, df_chk_depvar], axis=1)\n",
        "    df_chk_nums_concat = pd.concat([df_chk_nums, df_chk_pd_depvar], axis=1)\n",
        "\n",
        "\n",
        "    df_chk_chars_concat['DF_RECS_CNT'] = 1\n",
        "    df_chk_nums_concat['DF_RECS_CNT'] = 1\n",
        "\n",
        "    df_chk_chars_concat.fillna(\"BLANK\", inplace=True)\n",
        "    df_chk_nums_concat.fillna(-999999, inplace=True)\n",
        "\n",
        "\n",
        "    writer1 = pd.ExcelWriter(output_file_nm, engine='xlsxwriter')\n",
        "\n",
        "    summ_tab_df.to_excel(writer1, sheet_name=\"Summary\", startcol=2 , startrow=5 , index=False)\n",
        "    wbook = writer1.book\n",
        "    wsheet = writer1.sheets[\"Summary\"]\n",
        "    wsheet.set_column(2, 3, 25)\n",
        "    summ_range_strt = xl_rowcol_to_cell(5, 2)\n",
        "    summ_range_end = xl_rowcol_to_cell(5+len(summ_tab_df), 2)\n",
        "    b_range_var = summ_range_strt+':'+summ_range_end\n",
        "    link_range_strt = xl_rowcol_to_cell(5, 3)\n",
        "    link_range_end = xl_rowcol_to_cell(5+len(summ_tab_df), 3)\n",
        "    link_range_var = link_range_strt+':'+link_range_end\n",
        "    b_format=wbook.add_format({\n",
        "                'border':1,\n",
        "                'align':'center',\n",
        "                'font_size':10\n",
        "    })\n",
        "\n",
        "    link_format=wbook.add_format({'border':1,\n",
        "                'align':'center',\n",
        "                'font_size':9\n",
        "    })\n",
        "    link_format.set_font_color('blue')\n",
        "\n",
        "    wsheet.conditional_format(b_range_var, { 'type' : 'no_blanks' , 'format' : b_format } )\n",
        "    wsheet.conditional_format(link_range_var, { 'type' : 'no_blanks' , 'format' : link_format } )\n",
        "    wsheet.hide_gridlines(2)\n",
        "    wsheet.set_zoom(80)\n",
        "\n",
        "\n",
        "    for i in varlist_char:\n",
        "        if i != depvar and df_chk_chars_concat[i].unique().shape[0]<max_cat:\n",
        "            DF_CHAR = char_var_tbl(df_chk_chars_concat,depvar,i,writer1)\n",
        "            # DF_CHAR = DF_CHAR.sort_values([\"DF_RECS_CNT\"], ascending=False)\n",
        "            export_to_xl(DF_CHAR,var_name=i,writer=writer1)\n",
        "\n",
        "    for i in varlist_num :\n",
        "        if i != depvar:\n",
        "            DF_NUM = num_var_tbl(df_chk_nums_concat,depvar,i,writer1)\n",
        "            export_to_xl(DF_NUM,var_name=i,writer=writer1)\n",
        "\n",
        "    if MODEL_PERIOD_FLAG:\n",
        "        pd_tag_array = df_chk[MODEL_PERIOD_FLAG].unique()[:5]\n",
        "        pd_char_iter = 1\n",
        "\n",
        "        for j in pd_tag_array :\n",
        "            df_chars_pd_filt = df_chk_chars_concat[df_chk_chars_concat[MODEL_PERIOD_FLAG]==j]\n",
        "            for i2 in varlist_char:\n",
        "                print(i2)\n",
        "                if i2 != depvar and df_chars_pd_filt[i2].unique().shape[0]<max_cat :\n",
        "                    DF_CHAR = char_var_tbl(df_chars_pd_filt,depvar,i2,writer1)\n",
        "                    # DF_CHAR = DF_CHAR.sort_values(\"DF_RECS_CNT\", ascending=False)\n",
        "                    export_to_xl(DF_CHAR,var_name=i2,writer=writer1,iterator=pd_char_iter,pd_var=j)\n",
        "\n",
        "            df_num_pd_filt = df_chk_nums_concat[df_chk_nums_concat[MODEL_PERIOD_FLAG]==j]\n",
        "            for i2 in varlist_num:\n",
        "                print(i2)\n",
        "                if i2 != depvar :\n",
        "                    DF_NUM = num_var_tbl(df_num_pd_filt,depvar,i2,writer1)\n",
        "                    export_to_xl(DF_NUM,var_name=i2,writer=writer1,iterator=pd_char_iter,pd_var=j)\n",
        "            pd_char_iter = pd_char_iter+1\n",
        "        writer1.close()\n",
        "    else :\n",
        "        writer1.close()\n",
        "    print(\"Done!\")"
      ],
      "metadata": {
        "id": "oNGBhNFgMKJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the input data set for which bivariate analysis is to be done\n",
        "input_df = cdf1  # use encoding='unicode_escape' in case of unicode utf-8 error\n",
        "\n",
        "# Define the 'Dependent'/'Event' variable below:\n",
        "depvar = 'target1'\n",
        "\n",
        "#[OPTIONAL] : define the 'Period Variable ( E.g. variable with values \"01_TRAIN\"; \"02_VALIDATE\", \"03_OOT\") below ;\n",
        "Model_flag = 'tag'\n",
        "\n",
        "#output file name below:\n",
        "output_file_nm = r\"cdf_Bivariate_report_tagged.xlsx\"\n",
        "\n",
        "# [Function Call] !Remove Model_flag from arguments in function below, if it is not present in data/ not defined above\n",
        "bivariate(input_df,depvar,output_file_nm,MODEL_PERIOD_FLAG=Model_flag)\n",
        "# bivariate(input_df,depvar,output_file_nm)\n"
      ],
      "metadata": {
        "id": "K3UVWtCUMqO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cdf1['education_coded_in234'] = np.where(cdf1['education_coded'].isin([2,3,4]), 1, 0)\n",
        "cdf1.head()"
      ],
      "metadata": {
        "id": "nx00H8NGx-cS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cdf1 = cdf1.drop(['education_coded','NETMONTHLYINCOME'], axis=1)\n",
        "cdf1.shape"
      ],
      "metadata": {
        "id": "KiSFyKL4yeMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cdf1.columns"
      ],
      "metadata": {
        "id": "nEKSMOvYx3r6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preprocessing :::\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RUAZlo7VmnC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cdf2 = cdf1.copy()\n",
        "cdf2.head()"
      ],
      "metadata": {
        "id": "kQFoRhdZ3TQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### train-test split :::"
      ],
      "metadata": {
        "id": "W1wJ_-muB_ji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate train and test data\n",
        "cdf2_train = cdf2[cdf2['tag'] == 'train'].copy()\n",
        "cdf2_test = cdf2[cdf2['tag'] == 'test'].copy()"
      ],
      "metadata": {
        "id": "nXhxiUBBBVeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate event rate for training set\n",
        "event_rate_train = cdf2_train['target1'].mean()\n",
        "\n",
        "# Calculate event rate for test set\n",
        "event_rate_test = cdf2_test['target1'].mean()\n",
        "\n",
        "print(f\"Event Rate in Training Set: {event_rate_train:.4f}\")\n",
        "print(f\"Event Rate in Test Set: {event_rate_test:.4f}\")"
      ],
      "metadata": {
        "id": "1yNULc8YS01D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Outlier treatment - clip with 0.01th and 99th percentile values :::"
      ],
      "metadata": {
        "id": "ra6HzyqyCDyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "# Identify numerical features based on the number of unique values\n",
        "numerical_cols = [col for col in cdf2.columns if cdf2[col].nunique() > 20]\n",
        "\n",
        "# Calculate the number of rows needed for a 3-column layout\n",
        "n_cols = 3\n",
        "n_rows = math.ceil(len(numerical_cols) / n_cols)\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
        "axes = axes.flatten() # Flatten the 2D array of axes for easy iteration\n",
        "\n",
        "# Plot boxplots for each numerical feature\n",
        "for i, col in enumerate(numerical_cols):\n",
        "    sns.boxplot(y=cdf2[col], ax=axes[i])  # Changed x to y\n",
        "    axes[i].set_title(f'Boxplot of {col}')\n",
        "    axes[i].set_ylabel(col) # Changed xlabel to ylabel\n",
        "\n",
        "# Hide any unused subplots\n",
        "for j in range(i + 1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TxIj3Ol5_OjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify numerical features based on the number of unique values\n",
        "numerical_cols = [col for col in cdf2_train.columns if cdf2_train[col].nunique() > 20]\n",
        "\n",
        "# Calculate 1st and 99th percentiles for each numerical feature in the training set\n",
        "p01 = cdf2_train[numerical_cols].quantile(0.01)\n",
        "p99 = cdf2_train[numerical_cols].quantile(0.99)\n",
        "\n",
        "# Clip the values in both train and test sets\n",
        "cdf2_train[numerical_cols] = cdf2_train[numerical_cols].clip(lower=p01, upper=p99, axis=1)\n",
        "cdf2_test[numerical_cols] = cdf2_test[numerical_cols].clip(lower=p01, upper=p99, axis=1)\n",
        "\n",
        "print(\"Train set after clipping:\")\n",
        "display(cdf2_train.head())\n",
        "\n",
        "print(\"\\nTest set after clipping:\")\n",
        "display(cdf2_test.head())"
      ],
      "metadata": {
        "id": "-nqzhwfsBX8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "# Identify numerical features based on the number of unique values\n",
        "numerical_cols = [col for col in cdf2.columns if cdf2[col].nunique() > 20]\n",
        "\n",
        "# Calculate the number of rows needed for a 3-column layout\n",
        "n_cols = 3\n",
        "n_rows = math.ceil(len(numerical_cols) / n_cols)\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
        "axes = axes.flatten() # Flatten the 2D array of axes for easy iteration\n",
        "\n",
        "# Plot boxplots for each numerical feature\n",
        "for i, col in enumerate(numerical_cols):\n",
        "    sns.boxplot(y=cdf2_train[col], ax=axes[i])  # Changed x to y\n",
        "    axes[i].set_title(f'Boxplot of {col}')\n",
        "    axes[i].set_ylabel(col) # Changed xlabel to ylabel\n",
        "\n",
        "# Hide any unused subplots\n",
        "for j in range(i + 1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JwUCujY6HWbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing value imputation ::::"
      ],
      "metadata": {
        "id": "_jBpJKD1HJA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = [i for i in cdf2_train.columns if i not in ['tag','target1']]\n",
        "print(features)\n",
        "\n",
        "# Calculate median for each numerical feature in the training set\n",
        "medians = cdf2_train[features].median()\n",
        "\n",
        "# Fill missing values in both train and test sets with the calculated medians\n",
        "cdf2_train[features] = cdf2_train[features].fillna(medians)\n",
        "cdf2_test[features] = cdf2_test[features].fillna(medians)\n",
        "\n",
        "print(\"Missing values in train set after imputation:\")\n",
        "print(cdf2_train[features].isnull().sum())\n",
        "\n",
        "print(\"\\nMissing values in test set after imputation:\")\n",
        "print(cdf2_test[features].isnull().sum())"
      ],
      "metadata": {
        "id": "4ziH71e-HHk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Scalling : standard scalling :::"
      ],
      "metadata": {
        "id": "-xvll5bRC0-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "# Identify all features\n",
        "all_cols = cdf2.drop(['tag','target1'], axis=1).columns\n",
        "\n",
        "# Calculate the number of rows needed for a 3-column layout\n",
        "n_cols = 3\n",
        "n_rows = math.ceil(len(all_cols) / n_cols)\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
        "axes = axes.flatten() # Flatten the 2D array of axes for easy iteration\n",
        "\n",
        "# Plot density plots for each feature\n",
        "for i, col in enumerate(all_cols):\n",
        "    sns.kdeplot(x=cdf2_train[col], ax=axes[i])\n",
        "    axes[i].set_title(f'Density Plot of {col}')\n",
        "    axes[i].set_xlabel(col)\n",
        "\n",
        "# Hide any unused subplots\n",
        "for j in range(i + 1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mvjXqUr6DBhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OrdinalEncoder, LabelEncoder\n",
        "import joblib\n",
        "\n",
        "stdscaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the scaler on the training data\n",
        "cdf2_train[features] = stdscaler.fit_transform(cdf2_train[features])\n",
        "\n",
        "# Save the scaler\n",
        "joblib.dump(stdscaler, 'std_scaler.pkl')\n",
        "\n",
        "# Load the scaler and transform the test data\n",
        "loaded_scaler = joblib.load('std_scaler.pkl')\n",
        "cdf2_test[features] = loaded_scaler.transform(cdf2_test[features])"
      ],
      "metadata": {
        "id": "DHbJTkxOpiy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "# Identify all features\n",
        "all_cols = cdf2.drop(['tag','target1'], axis=1).columns\n",
        "\n",
        "# Calculate the number of rows needed for a 3-column layout\n",
        "n_cols = 3\n",
        "n_rows = math.ceil(len(all_cols) / n_cols)\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
        "axes = axes.flatten() # Flatten the 2D array of axes for easy iteration\n",
        "\n",
        "# Plot density plots for each feature\n",
        "for i, col in enumerate(all_cols):\n",
        "    sns.kdeplot(x=cdf2_train[col], ax=axes[i])\n",
        "    axes[i].set_title(f'Density Plot of {col}')\n",
        "    axes[i].set_xlabel(col)\n",
        "\n",
        "# Hide any unused subplots\n",
        "for j in range(i + 1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N2iqPzNxEX4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cdf2_train.head()"
      ],
      "metadata": {
        "id": "5Zy4fWMRsuSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Add a constant (intercept) to the features\n",
        "X_train_sm = sm.add_constant(cdf2_train[features].drop(['PL_Flag','time_since_recent_payment','HL_Flag','num_times_60p_dpd'], axis=1))\n",
        "\n",
        "# Fit the logistic regression model using statsmodels\n",
        "logit_model = sm.Logit(cdf2_train['target1'], X_train_sm)\n",
        "result = logit_model.fit()\n",
        "\n",
        "# Display the summary which includes coefficients and p-values\n",
        "print(result.summary())\n",
        "\n",
        "# # Define the models\n",
        "# lr_model = LogisticRegression(random_state=42)\n",
        "\n",
        "# Example hyperparameters for tree-based models\n",
        "dt_model = DecisionTreeClassifier(random_state=42, max_depth=4, min_samples_split=5)\n",
        "rf_model = RandomForestClassifier(random_state=42, n_estimators=50, max_depth=4, min_samples_leaf=5)\n",
        "xgb_model = XGBClassifier(random_state=42, n_estimators=50, learning_rate=0.1, max_depth=4)\n",
        "\n",
        "\n",
        "# Train the models\n",
        "# lr_model.fit(cdf2_train[features], cdf2_train['target1'])\n",
        "dt_model.fit(cdf2_train[features], cdf2_train['target1'])\n",
        "rf_model.fit(cdf2_train[features], cdf2_train['target1'])\n",
        "xgb_model.fit(cdf2_train[features], cdf2_train['target1'])\n",
        "\n",
        "print(\"Models trained successfully.\")"
      ],
      "metadata": {
        "id": "NGUroxBEuxQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "import statsmodels.api as sm # Import statsmodels\n",
        "\n",
        "models = {\n",
        "    \"Logistic Regression\": result, # Use the statsmodels result object\n",
        "    \"Decision Tree\": dt_model,\n",
        "    \"Random Forest\": rf_model,\n",
        "    \"XGBoost\": xgb_model\n",
        "}\n",
        "\n",
        "train_results = []\n",
        "test_results = []\n",
        "\n",
        "# Add a constant to the feature data for statsmodels prediction\n",
        "X_train_sm = sm.add_constant(cdf2_train[features].drop(['PL_Flag','time_since_recent_payment','HL_Flag','num_times_60p_dpd'], axis=1)) # Corrected feature list\n",
        "X_test_sm = sm.add_constant(cdf2_test[features].drop(['PL_Flag','time_since_recent_payment','HL_Flag','num_times_60p_dpd'], axis=1)) # Corrected feature list\n",
        "\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Use appropriate prediction method based on the model type\n",
        "    if name == \"Logistic Regression\":\n",
        "        y_pred_train = (model.predict(X_train_sm) > 0.5).astype(int) # Predict probabilities and convert to binary\n",
        "        y_proba_train = model.predict(X_train_sm) # statsmodels predict directly gives probabilities\n",
        "\n",
        "        y_pred_test = (model.predict(X_test_sm) > 0.5).astype(int)\n",
        "        y_proba_test = model.predict(X_test_sm)\n",
        "\n",
        "    else:\n",
        "        y_pred_train = model.predict(cdf2_train[features]) # Corrected feature list\n",
        "        y_proba_train = model.predict_proba(cdf2_train[features])[:, 1] # Corrected feature list\n",
        "\n",
        "        y_pred_test = model.predict(cdf2_test[features]) # Corrected feature list\n",
        "        y_proba_test = model.predict_proba(cdf2_test[features])[:, 1] # Corrected feature list\n",
        "\n",
        "\n",
        "    precision_train = precision_score(cdf2_train['target1'], y_pred_train)\n",
        "    recall_train = recall_score(cdf2_train['target1'], y_pred_train)\n",
        "    f1_train = f1_score(cdf2_train['target1'], y_pred_train)\n",
        "    auc_train = roc_auc_score(cdf2_train['target1'], y_proba_train)\n",
        "    tn_train, fp_train, fn_train, tp_train = confusion_matrix(cdf2_train['target1'], y_pred_train).ravel()\n",
        "\n",
        "\n",
        "    train_results.append({\n",
        "        \"Model\": name,\n",
        "        \"Precision\": precision_train,\n",
        "        \"Recall\": recall_train,\n",
        "        \"F1 Score\": f1_train,\n",
        "        \"AUC\": auc_train,\n",
        "        \"True Positives\": tp_train,\n",
        "        \"False Positives\": fp_train,\n",
        "        \"True Negatives\": tn_train,\n",
        "        \"False Negatives\": fn_train\n",
        "    })\n",
        "\n",
        "    precision_test = precision_score(cdf2_test['target1'], y_pred_test)\n",
        "    recall_test = recall_score(cdf2_test['target1'], y_pred_test)\n",
        "    f1_test = f1_score(cdf2_test['target1'], y_pred_test)\n",
        "    auc_test = roc_auc_score(cdf2_test['target1'], y_proba_test)\n",
        "    tn_test, fp_test, fn_test, tp_test = confusion_matrix(cdf2_test['target1'], y_pred_test).ravel()\n",
        "\n",
        "    test_results.append({\n",
        "        \"Model\": name,\n",
        "        \"Precision\": precision_test,\n",
        "        \"Recall\": recall_test,\n",
        "        \"F1 Score\": f1_test,\n",
        "        \"AUC\": auc_test,\n",
        "        \"True Positives\": tp_test,\n",
        "        \"False Positives\": fp_test,\n",
        "        \"True Negatives\": tn_test,\n",
        "        \"False Negatives\": fn_test\n",
        "    })\n",
        "\n",
        "train_results_df = pd.DataFrame(train_results)\n",
        "test_results_df = pd.DataFrame(test_results)\n",
        "\n",
        "print(\"Model Evaluation Metrics (Training Data):\")\n",
        "display(train_results_df)\n",
        "\n",
        "print(\"\\nModel Evaluation Metrics (Test Data):\")\n",
        "display(test_results_df)"
      ],
      "metadata": {
        "id": "6bJE-hiDvGM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd # Import pandas if not already imported\n",
        "\n",
        "def create_decile_report(y_true, y_proba, num_deciles=10):\n",
        "    \"\"\"\n",
        "    Generates a decile-wise report for a binary classification model.\n",
        "\n",
        "    Args:\n",
        "        y_true (pd.Series): True labels (0 or 1).\n",
        "        y_proba (pd.Series): Predicted probabilities for the positive class.\n",
        "        num_deciles (int): The number of deciles to create.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing the decile report.\n",
        "    \"\"\"\n",
        "    # Create a DataFrame with true labels and predicted probabilities\n",
        "    df = pd.DataFrame({'true_label': y_true, 'predicted_proba': y_proba})\n",
        "\n",
        "    # Sort by predicted probability in descending order\n",
        "    df = df.sort_values(by='predicted_proba', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    # Create deciles based on equal number of instances\n",
        "    df['decile'] = pd.qcut(df.index, q=num_deciles, labels=False, duplicates='drop') + 1\n",
        "\n",
        "    # Calculate metrics for each decile\n",
        "    decile_report = df.groupby('decile').agg(\n",
        "        population=('decile', 'count'),\n",
        "        events=('true_label', 'sum')\n",
        "    ).reset_index()\n",
        "\n",
        "    decile_report['non_events'] = decile_report['population'] - decile_report['events']\n",
        "    decile_report['event_rate'] = decile_report['events'] / decile_report['population']\n",
        "\n",
        "    # Cumulative calculations\n",
        "    decile_report['cumulative_population'] = decile_report['population'].cumsum()\n",
        "    decile_report['cumulative_events'] = decile_report['events'].cumsum()\n",
        "    decile_report['cumulative_non_events'] = decile_report['non_events'].cumsum()\n",
        "\n",
        "    # Calculate total events and non-events\n",
        "    total_events = decile_report['events'].sum()\n",
        "    total_non_events = decile_report['non_events'].sum()\n",
        "\n",
        "    # Calculate metrics\n",
        "    decile_report['event_capture_rate'] = decile_report['cumulative_events'] / total_events\n",
        "    decile_report['non_event_capture_rate'] = decile_report['cumulative_non_events'] / total_non_events\n",
        "    decile_report['lift'] = decile_report['event_capture_rate'] / (decile_report['cumulative_population'] / decile_report['population'].sum())\n",
        "    decile_report['gain'] = decile_report['event_capture_rate'] * 100\n",
        "    decile_report['precision'] = decile_report['cumulative_events'] / decile_report['cumulative_population']\n",
        "    decile_report['recall'] = decile_report['event_capture_rate']\n",
        "    decile_report['KS'] = decile_report['event_capture_rate'] - decile_report['non_event_capture_rate']\n",
        "\n",
        "\n",
        "    return decile_report[['decile', 'population', 'events', 'event_rate', 'lift', 'gain', 'precision', 'recall', 'KS']]\n",
        "\n",
        "# Define the models (Assuming result, dt_model, rf_model, xgb_model are already trained)\n",
        "models = {\n",
        "    \"Logistic Regression\": result,\n",
        "    \"Decision Tree\": dt_model,\n",
        "    \"Random Forest\": rf_model,\n",
        "    \"XGBoost\": xgb_model\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"Decile Report for {name} (Training Data):\")\n",
        "    # Get predicted probabilities for the training set\n",
        "    if name == \"Logistic Regression\":\n",
        "        # For statsmodels, predict() directly gives probabilities\n",
        "        y_proba_train = model.predict(sm.add_constant(cdf2_train[features].drop(['PL_Flag','time_since_recent_payment','HL_Flag','num_times_60p_dpd'], axis=1)))\n",
        "    else:\n",
        "        y_proba_train = model.predict_proba(cdf2_train[features])[:, 1]\n",
        "\n",
        "    # Create the decile report for the training data\n",
        "    decile_report_train = create_decile_report(cdf2_train['target1'], y_proba_train)\n",
        "\n",
        "    # Display only the first 3 deciles\n",
        "    display(decile_report_train.head(3))\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "Vh79b9ghN_LU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, model in models.items():\n",
        "    print(f\"Decile Report for {name} (Test Data):\")\n",
        "    # Get predicted probabilities for the training set\n",
        "    if name == \"Logistic Regression\":\n",
        "        # For statsmodels, predict() directly gives probabilities\n",
        "        y_proba_test = model.predict(sm.add_constant(cdf2_test[features].drop(['PL_Flag','time_since_recent_payment','HL_Flag','num_times_60p_dpd'], axis=1)))\n",
        "    else:\n",
        "        y_proba_test = model.predict_proba(cdf2_test[features])[:, 1]\n",
        "\n",
        "    # Create the decile report for the training data\n",
        "    decile_report_test = create_decile_report(cdf2_test['target1'], y_proba_test)\n",
        "\n",
        "    # Display only the first 3 deciles\n",
        "    display(decile_report_test.head(3))\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "KW-k6W4rP2eZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "waA6xdj6QBFe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}